{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQM8pwuieY04Kc+As6HLj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melinakubiana/Meta-Ai/blob/main/Melina_Kubiana_Meta_Ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRZ4PligvTE2",
        "outputId": "56bda04f-15bf-47e4-9953-f6eb2e5a8296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Installation complete\n"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "  llama-index \\\n",
        "  llama-index-llms-openrouter \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-packs-fusion-retriever \\\n",
        "  sentence-transformers \\\n",
        "  nest-asyncio \\\n",
        "  requests\n",
        "\n",
        "print(\"âœ… Installation complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Ask for your OpenRouter API key (input is hidden like a password)\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter API key: \")\n",
        "\n",
        "# Configure the LLM (Meta Llama via OpenRouter)\n",
        "llm = OpenRouter(\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct:free\",\n",
        "    max_tokens=512,\n",
        "    temperature=0.1,  # Low = more precise, less â€œcreativeâ€\n",
        "    timeout=60,\n",
        "    system_prompt=(\n",
        "        \"You are an expert RAG system that answers ONLY using the provided context. \"\n",
        "        \"Never hallucinate. Never guess. If the answer is not in the context, say so. \"\n",
        "        \"Provide short, clear, factual responses with 2â€“4 evidence bullets.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Configure the embedding model\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Register both with LlamaIndex settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print(\"âœ… AI model and settings are ready to use\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceaCHlCWxgsr",
        "outputId": "26315632-3848-46bf-effd-93908c2a4c3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenRouter API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AI model and settings are ready to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
        "    \"\"\"\n",
        "    Download a PDF from a Google Drive sharing link and save it locally.\n",
        "    \"\"\"\n",
        "    # Try pattern: /d/<FILE_ID>/\n",
        "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "    else:\n",
        "        # Try pattern: ?id=<FILE_ID>\n",
        "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"âŒ Could not extract file ID from the link.\")\n",
        "\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    print(f\"ğŸ“¥ Downloading PDF (file ID {file_id})...\")\n",
        "\n",
        "    resp = requests.get(download_url)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "\n",
        "    print(f\"âœ… PDF downloaded â†’ {save_path}\")\n",
        "\n",
        "# Ask for the Drive link\n",
        "drive_link = input(\"ğŸ“Œ Paste your Google Drive PDF link here: \").strip()\n",
        "\n",
        "# Make sure the data folder exists\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Local path for the PDF\n",
        "pdf_path = os.path.join(DATA_DIR, \"source.pdf\")\n",
        "\n",
        "# Download the PDF\n",
        "download_pdf_from_drive(drive_link, pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZZr3Z0zyAFJ",
        "outputId": "57a9168e-5613-41d8-fdc5-e34f21b1288c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Œ Paste your Google Drive PDF link here: https://drive.google.com/file/d/17lhsQrEvxGhCnHUndCQL0MjnoQQTMLJN/view?usp=drive_link\n",
            "ğŸ“¥ Downloading PDF (file ID 17lhsQrEvxGhCnHUndCQL0MjnoQQTMLJN)...\n",
            "âœ… PDF downloaded â†’ data/source.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Load the PDF as a document\n",
        "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
        "print(f\"ğŸ“„ Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# Embedding model for semantic splitting (can reuse the same model name)\n",
        "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Create a semantic splitter\n",
        "parser = SemanticSplitterNodeParser(\n",
        "    buffer_size=3,\n",
        "    breakpoint_percentile_threshold=95,\n",
        "    embed_model=semantic_embed,\n",
        ")\n",
        "\n",
        "# Generate semantic nodes (chunks)\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Add simple metadata to each chunk\n",
        "for n in nodes:\n",
        "    n.metadata[\"source\"] = pdf_path\n",
        "    n.metadata[\"chunk_type\"] = \"semantic\"\n",
        "\n",
        "print(f\"ğŸ” Created {len(nodes)} high-quality semantic nodes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CHK4eKyyg9",
        "outputId": "462d0afd-912b-450c-c6d5-499ba470677a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 30 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Loaded 1 document(s).\n",
            "ğŸ” Created 2 high-quality semantic nodes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "# Download or load the Query Fusion pack\n",
        "QueryRewritingRetrieverPack = download_llama_pack(\n",
        "    \"QueryRewritingRetrieverPack\",\n",
        "    \"./query_rewriting_pack\",\n",
        ")\n",
        "\n",
        "# Create the advanced retriever using your nodes\n",
        "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
        "    nodes,                      # semantic chunks from Step 4\n",
        "    chunk_size=256,\n",
        "    vector_similarity_top_k=8,\n",
        "    fusion_similarity_top_k=8,\n",
        "    num_queries=6,              # number of query rewrites\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Advanced Query Fusion RAG Engine Ready!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAj7WmiEy441",
        "outputId": "72454274-706b-4400-ae59-1a8364b952ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Advanced Query Fusion RAG Engine Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_rag_run(question, retries=3):\n",
        "    \"\"\"\n",
        "    Run the RAG pipeline with basic retry logic.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            resp = query_rewriting_pack.run(question)\n",
        "\n",
        "            if resp is None or str(resp).strip() == \"\":\n",
        "                raise ValueError(\"Empty LLM response.\")\n",
        "\n",
        "            return resp\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error: {e}\")\n",
        "            print(f\"ğŸ” Retrying ({attempt+1}/{retries})...\")\n",
        "\n",
        "    return \"âŒ Could not generate a valid answer after retries.\"\n",
        "\n",
        "print(\"\\nRAG Interactive Mode\")\n",
        "print(\"Ask any question about your PDF.\")\n",
        "print(\"Type 'end' to exit.\\n\")\n",
        "\n",
        "# Interactive Q&A loop\n",
        "while True:\n",
        "    user_question = input(\"ğŸŸ¦ Enter your question: \").strip()\n",
        "\n",
        "    if user_question.lower() == \"end\":\n",
        "        print(\"\\nğŸ‘‹ Session ended.\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nğŸ” Retrieving answer...\\n\")\n",
        "\n",
        "    # Run the question through the RAG pipeline\n",
        "    response = safe_rag_run(user_question)\n",
        "\n",
        "    print(\"\\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "    print(\"â“ QUESTION:\")\n",
        "    print(user_question)\n",
        "    print(\"\\nğŸ§  ANSWER:\")\n",
        "    print(response)\n",
        "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LINWQ6TZzIiL",
        "outputId": "58a5a505-31d1-4cd0-d009-25b5eccec61e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RAG Interactive Mode\n",
            "Ask any question about your PDF.\n",
            "Type 'end' to exit.\n",
            "\n",
            "ğŸŸ¦ Enter your question: What are the main goals in this document?\n",
            "\n",
            "ğŸ” Retrieving answer...\n",
            "\n",
            "Generated queries:\n",
            "What are the primary objectives outlined in the document?\n",
            "What key targets are specified in the text?\n",
            "What are the main purposes and intentions stated in the document?\n",
            "What goals and outcomes are explicitly mentioned in the document?\n",
            "What are the central aims and desired results discussed in the document?\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "â“ QUESTION:\n",
            "What are the main goals in this document?\n",
            "\n",
            "ğŸ§  ANSWER:\n",
            "The main goals in this document are:\n",
            "* To express interest in joining an agricultural expenditure trip to Brazil\n",
            "* To learn from the University of SÃ£o Paulo's research in agriculture and biosciences\n",
            "* To gain knowledge and skills to strengthen community-based agricultural solutions\n",
            "* To become a small-scale farmer assessor and mentor for youth in the agricultural sector\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸŸ¦ Enter your question: end\n",
            "\n",
            "ğŸ‘‹ Session ended.\n"
          ]
        }
      ]
    }
  ]
}